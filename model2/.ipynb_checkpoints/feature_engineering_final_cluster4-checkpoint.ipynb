{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, sys, gc, time, warnings, pickle, psutil, random\n",
    "import datetime\n",
    "from math import ceil\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_memory_usage():\n",
    "    return np.round(psutil.Process(os.getpid()).memory_info()[0]/2.**30, 2) \n",
    "        \n",
    "def sizeof_fmt(num, suffix='B'):\n",
    "    for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:\n",
    "        if abs(num) < 1024.0:\n",
    "            return \"%3.1f%s%s\" % (num, unit, suffix)\n",
    "        num /= 1024.0\n",
    "    return \"%.1f%s%s\" % (num, 'Yi', suffix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                       df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Merging by concat to not lose dtypes\n",
    "def merge_by_concat(df1, df2, merge_on):\n",
    "    merged_gf = df1[merge_on]\n",
    "    merged_gf = merged_gf.merge(df2, on=merge_on, how='left')\n",
    "    new_columns = [col for col in list(merged_gf) if col not in merge_on]\n",
    "    df1 = pd.concat([df1, merged_gf[new_columns]], axis=1)\n",
    "    return df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET = 'sales'         # Our main target\n",
    "END_TRAIN = 1941         # Last day in train set, change this part for final\n",
    "MAIN_INDEX = ['id','d']  # We can identify item by these columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Main Data\n"
     ]
    }
   ],
   "source": [
    "print('Load Main Data')\n",
    "# Here are reafing all our data \n",
    "# without any limitations and dtype modification\n",
    "train_df = pd.read_csv('sales_train_evaluation.csv') # change this part for final\n",
    "prices_df = pd.read_csv('sell_prices.csv')\n",
    "calendar_df = pd.read_csv('calendar.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use cluster to remove some obs\n",
    "cluster=pd.read_csv('cluster4.csv') # change this part for final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cluster</th>\n",
       "      <th>item_id</th>\n",
       "      <th>0</th>\n",
       "      <th>7</th>\n",
       "      <th>14</th>\n",
       "      <th>21</th>\n",
       "      <th>28</th>\n",
       "      <th>35</th>\n",
       "      <th>42</th>\n",
       "      <th>49</th>\n",
       "      <th>...</th>\n",
       "      <th>1876</th>\n",
       "      <th>1883</th>\n",
       "      <th>1890</th>\n",
       "      <th>1897</th>\n",
       "      <th>1904</th>\n",
       "      <th>1911</th>\n",
       "      <th>1918</th>\n",
       "      <th>1925</th>\n",
       "      <th>1932</th>\n",
       "      <th>1939</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>FOODS_1_004</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.241116</td>\n",
       "      <td>1.857643</td>\n",
       "      <td>1.897480</td>\n",
       "      <td>1.285254</td>\n",
       "      <td>1.227945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>FOODS_1_009</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.037360</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.971359</td>\n",
       "      <td>2.727278</td>\n",
       "      <td>2.951438</td>\n",
       "      <td>2.876718</td>\n",
       "      <td>2.876718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>FOODS_1_010</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.422206</td>\n",
       "      <td>1.244430</td>\n",
       "      <td>2.666636</td>\n",
       "      <td>1.688870</td>\n",
       "      <td>1.911089</td>\n",
       "      <td>1.911089</td>\n",
       "      <td>1.466650</td>\n",
       "      <td>1.333318</td>\n",
       "      <td>1.377762</td>\n",
       "      <td>2.592563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>FOODS_1_012</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.203461</td>\n",
       "      <td>1.286679</td>\n",
       "      <td>1.325087</td>\n",
       "      <td>1.302682</td>\n",
       "      <td>1.277077</td>\n",
       "      <td>1.190658</td>\n",
       "      <td>1.465918</td>\n",
       "      <td>1.142648</td>\n",
       "      <td>1.091437</td>\n",
       "      <td>1.299482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>FOODS_1_014</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.760925</td>\n",
       "      <td>1.320694</td>\n",
       "      <td>1.700894</td>\n",
       "      <td>1.740915</td>\n",
       "      <td>1.660873</td>\n",
       "      <td>2.161136</td>\n",
       "      <td>1.720904</td>\n",
       "      <td>2.261188</td>\n",
       "      <td>2.221167</td>\n",
       "      <td>1.820957</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 280 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   cluster      item_id    0    7   14   21   28   35   42   49  ...  \\\n",
       "0        0  FOODS_1_004  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   \n",
       "1        0  FOODS_1_009  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   \n",
       "2        0  FOODS_1_010  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   \n",
       "3        0  FOODS_1_012  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   \n",
       "4        0  FOODS_1_014  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   \n",
       "\n",
       "       1876      1883      1890      1897      1904      1911      1918  \\\n",
       "0  0.000000  0.000000  0.000000  0.000000  0.000000  0.241116  1.857643   \n",
       "1  0.037360  0.000000  0.000000  0.000000  0.000000  0.971359  2.727278   \n",
       "2  1.422206  1.244430  2.666636  1.688870  1.911089  1.911089  1.466650   \n",
       "3  1.203461  1.286679  1.325087  1.302682  1.277077  1.190658  1.465918   \n",
       "4  1.760925  1.320694  1.700894  1.740915  1.660873  2.161136  1.720904   \n",
       "\n",
       "       1925      1932      1939  \n",
       "0  1.897480  1.285254  1.227945  \n",
       "1  2.951438  2.876718  2.876718  \n",
       "2  1.333318  1.377762  2.592563  \n",
       "3  1.142648  1.091437  1.299482  \n",
       "4  2.261188  2.221167  1.820957  \n",
       "\n",
       "[5 rows x 280 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster=cluster[['cluster', 'item_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    1700\n",
       "0     837\n",
       "1     353\n",
       "3     159\n",
       "Name: cluster, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster['cluster'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>dept_id</th>\n",
       "      <th>cat_id</th>\n",
       "      <th>store_id</th>\n",
       "      <th>state_id</th>\n",
       "      <th>d_1</th>\n",
       "      <th>d_2</th>\n",
       "      <th>d_3</th>\n",
       "      <th>d_4</th>\n",
       "      <th>...</th>\n",
       "      <th>d_1932</th>\n",
       "      <th>d_1933</th>\n",
       "      <th>d_1934</th>\n",
       "      <th>d_1935</th>\n",
       "      <th>d_1936</th>\n",
       "      <th>d_1937</th>\n",
       "      <th>d_1938</th>\n",
       "      <th>d_1939</th>\n",
       "      <th>d_1940</th>\n",
       "      <th>d_1941</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HOBBIES_1_001_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_001</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HOBBIES_1_002_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_002</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HOBBIES_1_003_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_003</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HOBBIES_1_004_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_004</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HOBBIES_1_005_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_005</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1947 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              id        item_id    dept_id   cat_id store_id  \\\n",
       "0  HOBBIES_1_001_CA_1_evaluation  HOBBIES_1_001  HOBBIES_1  HOBBIES     CA_1   \n",
       "1  HOBBIES_1_002_CA_1_evaluation  HOBBIES_1_002  HOBBIES_1  HOBBIES     CA_1   \n",
       "2  HOBBIES_1_003_CA_1_evaluation  HOBBIES_1_003  HOBBIES_1  HOBBIES     CA_1   \n",
       "3  HOBBIES_1_004_CA_1_evaluation  HOBBIES_1_004  HOBBIES_1  HOBBIES     CA_1   \n",
       "4  HOBBIES_1_005_CA_1_evaluation  HOBBIES_1_005  HOBBIES_1  HOBBIES     CA_1   \n",
       "\n",
       "  state_id  d_1  d_2  d_3  d_4  ...  d_1932  d_1933  d_1934  d_1935  d_1936  \\\n",
       "0       CA    0    0    0    0  ...       2       4       0       0       0   \n",
       "1       CA    0    0    0    0  ...       0       1       2       1       1   \n",
       "2       CA    0    0    0    0  ...       1       0       2       0       0   \n",
       "3       CA    0    0    0    0  ...       1       1       0       4       0   \n",
       "4       CA    0    0    0    0  ...       0       0       0       2       1   \n",
       "\n",
       "   d_1937  d_1938  d_1939  d_1940  d_1941  \n",
       "0       0       3       3       0       1  \n",
       "1       0       0       0       0       0  \n",
       "2       0       2       3       0       1  \n",
       "3       1       3       0       2       6  \n",
       "4       0       0       2       1       0  \n",
       "\n",
       "[5 rows x 1947 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df=train_df.merge(cluster, on='item_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>dept_id</th>\n",
       "      <th>cat_id</th>\n",
       "      <th>store_id</th>\n",
       "      <th>state_id</th>\n",
       "      <th>d_1</th>\n",
       "      <th>d_2</th>\n",
       "      <th>d_3</th>\n",
       "      <th>d_4</th>\n",
       "      <th>...</th>\n",
       "      <th>d_1933</th>\n",
       "      <th>d_1934</th>\n",
       "      <th>d_1935</th>\n",
       "      <th>d_1936</th>\n",
       "      <th>d_1937</th>\n",
       "      <th>d_1938</th>\n",
       "      <th>d_1939</th>\n",
       "      <th>d_1940</th>\n",
       "      <th>d_1941</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HOBBIES_1_001_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_001</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HOBBIES_1_002_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_002</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HOBBIES_1_003_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_003</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HOBBIES_1_004_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_004</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HOBBIES_1_005_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_005</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1948 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              id        item_id    dept_id   cat_id store_id  \\\n",
       "0  HOBBIES_1_001_CA_1_evaluation  HOBBIES_1_001  HOBBIES_1  HOBBIES     CA_1   \n",
       "1  HOBBIES_1_002_CA_1_evaluation  HOBBIES_1_002  HOBBIES_1  HOBBIES     CA_1   \n",
       "2  HOBBIES_1_003_CA_1_evaluation  HOBBIES_1_003  HOBBIES_1  HOBBIES     CA_1   \n",
       "3  HOBBIES_1_004_CA_1_evaluation  HOBBIES_1_004  HOBBIES_1  HOBBIES     CA_1   \n",
       "4  HOBBIES_1_005_CA_1_evaluation  HOBBIES_1_005  HOBBIES_1  HOBBIES     CA_1   \n",
       "\n",
       "  state_id  d_1  d_2  d_3  d_4  ...  d_1933  d_1934  d_1935  d_1936  d_1937  \\\n",
       "0       CA    0    0    0    0  ...       4       0       0       0       0   \n",
       "1       CA    0    0    0    0  ...       1       2       1       1       0   \n",
       "2       CA    0    0    0    0  ...       0       2       0       0       0   \n",
       "3       CA    0    0    0    0  ...       1       0       4       0       1   \n",
       "4       CA    0    0    0    0  ...       0       0       2       1       0   \n",
       "\n",
       "   d_1938  d_1939  d_1940  d_1941  cluster  \n",
       "0       3       3       0       1        0  \n",
       "1       0       0       0       0        2  \n",
       "2       2       3       0       1        1  \n",
       "3       3       0       2       6        2  \n",
       "4       0       2       1       0        2  \n",
       "\n",
       "[5 rows x 1948 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    17000\n",
       "0     8370\n",
       "1     3530\n",
       "3     1590\n",
       "Name: cluster, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['cluster'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create Grid\n",
      "Train rows: 30490 59181090\n",
      "    Original grid_df:   4.0GiB\n"
     ]
    }
   ],
   "source": [
    "print('Create Grid')\n",
    "\n",
    "# We can tranform horizontal representation \n",
    "# to vertical \"view\"\n",
    "# Our \"index\" will be 'id','item_id','dept_id','cat_id','store_id','state_id'\n",
    "# and labels are 'd_' coulmns\n",
    "\n",
    "index_columns = ['id','item_id','dept_id','cat_id','store_id','state_id','cluster']\n",
    "grid_df = pd.melt(train_df, \n",
    "                  id_vars = index_columns, \n",
    "                  var_name = 'd', \n",
    "                  value_name = TARGET)\n",
    "\n",
    "# If we look on train_df we see that \n",
    "# we don't have a lot of traning rows\n",
    "# but each day can provide more train data\n",
    "print('Train rows:', len(train_df), len(grid_df))\n",
    "\n",
    "# To be able to make predictions\n",
    "# we need to add \"test set\" to our grid\n",
    "add_grid = pd.DataFrame()\n",
    "for i in range(1,29):\n",
    "    temp_df = train_df[index_columns]\n",
    "    temp_df = temp_df.drop_duplicates()\n",
    "    temp_df['d'] = 'd_'+ str(END_TRAIN+i)\n",
    "    temp_df[TARGET] = np.nan\n",
    "    add_grid = pd.concat([add_grid,temp_df])\n",
    "\n",
    "grid_df = pd.concat([grid_df,add_grid])\n",
    "grid_df = grid_df.reset_index(drop=True)\n",
    "\n",
    "# Remove some temoprary DFs\n",
    "del temp_df, add_grid\n",
    "\n",
    "# We will not need original train_df\n",
    "# anymore and can remove it\n",
    "del train_df\n",
    "\n",
    "# You don't have to use df = df construction\n",
    "# you can use inplace=True instead.\n",
    "# like this\n",
    "# grid_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Let's check our memory usage\n",
    "print(\"{:>20}: {:>8}\".format('Original grid_df',sizeof_fmt(grid_df.memory_usage(index=True).sum())))\n",
    "\n",
    "# We can free some memory \n",
    "# by converting \"strings\" to categorical\n",
    "# it will not affect merging and \n",
    "# we will not lose any valuable data\n",
    "for col in ['id','item_id','dept_id','cat_id','store_id','state_id']:\n",
    "    grid_df[col] = grid_df[col].astype('category')\n",
    "\n",
    "# Let's check again memory usage\n",
    "print(\"{:>20}: {:>8}\".format('Reduced grid_df',sizeof_fmt(grid_df.memory_usage(index=True).sum())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_df.head() #1-1941:train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_df.tail() #1942-1969:test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_df['day']=grid_df['d'].apply(lambda x:int(x.split('_')[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove cluster leading zero or few obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_df['cluster'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_df=grid_df.drop(grid_df[(grid_df['cluster']==1)&(grid_df['day']<=500)].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "60034810-58269810"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_df[grid_df['cluster']==1].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_df[grid_df['cluster']==0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_df=grid_df.drop(grid_df[(grid_df['cluster']==0)&(grid_df['day']<=250)].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "58269810-56177310"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_df[grid_df['cluster']==0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_df[grid_df['cluster']==3].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_df=grid_df.drop(grid_df[(grid_df['cluster']==3)&(grid_df['day']<=1000)].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "56177310-54587310"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_df[grid_df['cluster']==0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "60034810-54587310"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Release week')\n",
    "\n",
    "# It seems that leadings zero values\n",
    "# in each train_df item row\n",
    "# are not real 0 sales but mean\n",
    "# absence for the item in the store\n",
    "# we can safe some memory by removing\n",
    "# such zeros\n",
    "\n",
    "# Prices are set by week\n",
    "# so it we will have not very accurate release week \n",
    "release_df = prices_df.groupby(['store_id','item_id'])['wm_yr_wk'].agg(['min']).reset_index()\n",
    "release_df.columns = ['store_id','item_id','release']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "release_df.head() # release: the first wm_yr_wk has price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can merge release_df\n",
    "grid_df = merge_by_concat(grid_df, release_df, ['store_id','item_id'])\n",
    "del release_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_df[(grid_df['store_id']=='CA_1') & (grid_df['item_id']=='FOODS_1_001')].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to remove some \"zeros\" rows\n",
    "# from grid_df \n",
    "# to do it we need wm_yr_wk column\n",
    "# let's merge partly calendar_df to have it\n",
    "grid_df = merge_by_concat(grid_df, calendar_df[['wm_yr_wk','d']], ['d'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_df[(grid_df['store_id']=='CA_1') & (grid_df['item_id']=='FOODS_1_001')].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_df[(grid_df['store_id']=='CA_1') & (grid_df['item_id']=='FOODS_1_001')].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can cutoff some rows \n",
    "# and safe memory \n",
    "grid_df = grid_df[grid_df['wm_yr_wk']>=grid_df['release']]\n",
    "grid_df = grid_df.reset_index(drop=True)\n",
    "\n",
    "# Let's check our memory usage\n",
    "print(\"{:>20}: {:>8}\".format('Original grid_df',sizeof_fmt(grid_df.memory_usage(index=True).sum())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_df[(grid_df['store_id']=='CA_1') & (grid_df['item_id']=='FOODS_1_001')].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_df.shape  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "54587310-47617813"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_df.drop('day', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Should we keep release week \n",
    "# as one of the features?\n",
    "# Only good CV can give the answer.\n",
    "# Let's minify the release values.\n",
    "# Min transformation will not help here \n",
    "# as int16 -> Integer (-32768 to 32767)\n",
    "# and our grid_df['release'].max() serves for int16\n",
    "# but we have have an idea how to transform \n",
    "# other columns in case we will need it\n",
    "grid_df['release'] = grid_df['release'] - grid_df['release'].min()\n",
    "grid_df['release'] = grid_df['release'].astype(np.int16)\n",
    "\n",
    "# Let's check again memory usage\n",
    "print(\"{:>20}: {:>8}\".format('Reduced grid_df',sizeof_fmt(grid_df.memory_usage(index=True).sum())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_df[(grid_df['store_id']=='CA_1') & (grid_df['item_id']=='FOODS_1_001')].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_df[(grid_df['store_id']=='CA_1') & (grid_df['item_id']=='FOODS_1_001')].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Save Part 1')\n",
    "\n",
    "# We have our BASE grid ready\n",
    "# and can save it as pickle file\n",
    "# for future use (model training)\n",
    "grid_df.to_pickle('grid_part_1.pkl')\n",
    "\n",
    "print('Size:', grid_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add price feature\n",
    "prices_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Prices')\n",
    "\n",
    "# We can do some basic aggregations\n",
    "prices_df['price_max'] = prices_df.groupby(['store_id','item_id'])['sell_price'].transform('max')\n",
    "prices_df['price_min'] = prices_df.groupby(['store_id','item_id'])['sell_price'].transform('min')\n",
    "prices_df['price_std'] = prices_df.groupby(['store_id','item_id'])['sell_price'].transform('std')\n",
    "prices_df['price_mean'] = prices_df.groupby(['store_id','item_id'])['sell_price'].transform('mean')\n",
    "\n",
    "# and do price normalization (min/max scaling)\n",
    "prices_df['price_norm'] = prices_df['sell_price']/prices_df['price_max']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# #price_rank_dept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# price_rank_dept\n",
    "prices_df[\"dept\"]=prices_df[\"item_id\"].apply(lambda x: x.split('_'))\n",
    "prices_df[\"dept\"]=prices_df[\"dept\"].apply(lambda x: str(x[0:2]))\n",
    "\n",
    "cat=prices_df[\"dept\"].value_counts().index\n",
    "\n",
    "a=pd.DataFrame()\n",
    "k=10\n",
    "for i in range(len(cat))    :\n",
    "    temp=pd.cut(prices_df[prices_df[\"dept\"]==cat[i]]['sell_price'],k,labels = range(k))\n",
    "    a=pd.concat([a,temp])\n",
    "a.columns=[\"price_rank_dept\"]\n",
    "a.sort_index(inplace=True)\n",
    "\n",
    "prices_df=pd.concat([prices_df,a],axis=1)\n",
    "prices_df[\"price_rank_dept\"]=prices_df[\"price_rank_dept\"].astype(\"category\")\n",
    "prices_df.drop([\"dept\"],axis=1,inplace=True)\n",
    "del a,cat,k\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prices_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some items are can be inflation dependent\n",
    "# and some items are very \"stable\"\n",
    "# check how many unique values each group has\n",
    "prices_df['price_nunique'] = prices_df.groupby(['store_id','item_id'])['sell_price'].transform('nunique')\n",
    "prices_df['item_nunique'] = prices_df.groupby(['store_id','sell_price'])['item_id'].transform('nunique')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prices_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I would like some \"rolling\" aggregations\n",
    "# but would like months and years as \"window\"\n",
    "calendar_prices = calendar_df[['wm_yr_wk','month','year']]\n",
    "calendar_prices = calendar_prices.drop_duplicates(subset=['wm_yr_wk'])\n",
    "prices_df = prices_df.merge(calendar_prices[['wm_yr_wk','month','year']], on=['wm_yr_wk'], how='left')\n",
    "del calendar_prices\n",
    "\n",
    "# Now we can add price \"momentum\" (some sort of)\n",
    "# Shifted by week \n",
    "# by month mean\n",
    "# by year mean\n",
    "prices_df['price_momentum'] = prices_df['sell_price']/prices_df.groupby(['store_id','item_id'])['sell_price'].transform(lambda x: x.shift(1))\n",
    "prices_df['price_momentum_m'] = prices_df['sell_price']/prices_df.groupby(['store_id','item_id','month'])['sell_price'].transform('mean')\n",
    "prices_df['price_momentum_y'] = prices_df['sell_price']/prices_df.groupby(['store_id','item_id','year'])['sell_price'].transform('mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# weather-box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## https://zh.weatherspark.com/y/\n",
    "# 'temperature_high' \n",
    "# 'temperature_low' \n",
    "# temperature: \n",
    "# 极冷 0\n",
    "# 冰冻 1\n",
    "# 很冷 2\n",
    "# 寒冷 3\n",
    "# 凉爽 4\n",
    "# 舒适 5\n",
    "# 暖和 6\n",
    "# 热 7\n",
    "# 热 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_mask=(prices_df[\"store_id\"]==\"CA_1\")|(prices_df[\"store_id\"]==\"CA_2\")|(prices_df[\"store_id\"]==\"CA_3\")|(prices_df[\"store_id\"]==\"CA_4\")\n",
    "tx_mask=(prices_df[\"store_id\"]==\"TX_1\")|(prices_df[\"store_id\"]==\"TX_2\")|(prices_df[\"store_id\"]==\"TX_3\")\n",
    "wi_mask=(prices_df[\"store_id\"]==\"WI_1\")|(prices_df[\"store_id\"]==\"WI_2\")|(prices_df[\"store_id\"]==\"WI_3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#WEATHER PARMS\n",
    "prices_df['temperature_high']=np.nan\n",
    "prices_df['temperature_con']=np.nan\n",
    "prices_df['rainfall_m']=np.nan\n",
    "prices_df['snow_m']=np.nan\n",
    "\n",
    "## wi_mask \n",
    "# temperature_con\n",
    "\n",
    "prices_df.loc[prices_df[(wi_mask&(prices_df[\"month\"]==1))].index,'temperature_con']=1\n",
    "prices_df.loc[prices_df[(wi_mask&(prices_df[\"month\"]==2))].index,'temperature_con']=2\n",
    "prices_df.loc[prices_df[(wi_mask&(prices_df[\"month\"]==3))].index,'temperature_con']=2.5\n",
    "prices_df.loc[prices_df[(wi_mask&(prices_df[\"month\"]==4))].index,'temperature_con']=3.5\n",
    "prices_df.loc[prices_df[(wi_mask&(prices_df[\"month\"]==5))].index,'temperature_con']=4.5\n",
    "prices_df.loc[prices_df[(wi_mask&(prices_df[\"month\"]==6))].index,'temperature_con']=5.5\n",
    "prices_df.loc[prices_df[(wi_mask&(prices_df[\"month\"]==7))].index,'temperature_con']=6\n",
    "prices_df.loc[prices_df[(wi_mask&(prices_df[\"month\"]==8))].index,'temperature_con']=6\n",
    "prices_df.loc[prices_df[(wi_mask&(prices_df[\"month\"]==9))].index,'temperature_con']=5\n",
    "prices_df.loc[prices_df[(wi_mask&(prices_df[\"month\"]==10))].index,'temperature_con']=4\n",
    "prices_df.loc[prices_df[(wi_mask&(prices_df[\"month\"]==11))].index,'temperature_con']=3\n",
    "prices_df.loc[prices_df[(wi_mask&(prices_df[\"month\"]==12))].index,'temperature_con']=2\n",
    "\n",
    "# temperature_high\n",
    "\n",
    "prices_df.loc[prices_df[(wi_mask&(prices_df[\"month\"]==1))].index,'temperature_high']=0.5\n",
    "prices_df.loc[prices_df[(wi_mask&(prices_df[\"month\"]==2))].index,'temperature_high']=2\n",
    "prices_df.loc[prices_df[(wi_mask&(prices_df[\"month\"]==3))].index,'temperature_high']=6\n",
    "prices_df.loc[prices_df[(wi_mask&(prices_df[\"month\"]==4))].index,'temperature_high']=12\n",
    "prices_df.loc[prices_df[(wi_mask&(prices_df[\"month\"]==5))].index,'temperature_high']=18\n",
    "prices_df.loc[prices_df[(wi_mask&(prices_df[\"month\"]==6))].index,'temperature_high']=(21+27)/2\n",
    "prices_df.loc[prices_df[(wi_mask&(prices_df[\"month\"]==7))].index,'temperature_high']=27\n",
    "prices_df.loc[prices_df[(wi_mask&(prices_df[\"month\"]==8))].index,'temperature_high']=25\n",
    "prices_df.loc[prices_df[(wi_mask&(prices_df[\"month\"]==9))].index,'temperature_high']=22\n",
    "prices_df.loc[prices_df[(wi_mask&(prices_df[\"month\"]==10))].index,'temperature_high']=(17+12)/2\n",
    "prices_df.loc[prices_df[(wi_mask&(prices_df[\"month\"]==11))].index,'temperature_high']=(12+4)/2\n",
    "prices_df.loc[prices_df[(wi_mask&(prices_df[\"month\"]==12))].index,'temperature_high']=(4+0)/2\n",
    "\n",
    "# rainfall_m\n",
    "\n",
    "prices_df.loc[prices_df[(wi_mask&(prices_df[\"month\"]==1))].index,'rainfall_m']=16\n",
    "prices_df.loc[prices_df[(wi_mask&(prices_df[\"month\"]==2))].index,'rainfall_m']=21\n",
    "prices_df.loc[prices_df[(wi_mask&(prices_df[\"month\"]==3))].index,'rainfall_m']=42\n",
    "prices_df.loc[prices_df[(wi_mask&(prices_df[\"month\"]==4))].index,'rainfall_m']=72\n",
    "prices_df.loc[prices_df[(wi_mask&(prices_df[\"month\"]==5))].index,'rainfall_m']=80\n",
    "prices_df.loc[prices_df[(wi_mask&(prices_df[\"month\"]==6))].index,'rainfall_m']=86\n",
    "prices_df.loc[prices_df[(wi_mask&(prices_df[\"month\"]==7))].index,'rainfall_m']=80\n",
    "prices_df.loc[prices_df[(wi_mask&(prices_df[\"month\"]==8))].index,'rainfall_m']=82\n",
    "prices_df.loc[prices_df[(wi_mask&(prices_df[\"month\"]==9))].index,'rainfall_m']=76\n",
    "prices_df.loc[prices_df[(wi_mask&(prices_df[\"month\"]==10))].index,'rainfall_m']=63\n",
    "prices_df.loc[prices_df[(wi_mask&(prices_df[\"month\"]==11))].index,'rainfall_m']=50\n",
    "prices_df.loc[prices_df[(wi_mask&(prices_df[\"month\"]==12))].index,'rainfall_m']=31\n",
    "\n",
    "# snow_m\n",
    "\n",
    "prices_df.loc[prices_df[(wi_mask&(prices_df[\"month\"]==1))].index,'snow_m']=11\n",
    "prices_df.loc[prices_df[(wi_mask&(prices_df[\"month\"]==2))].index,'snow_m']=10\n",
    "prices_df.loc[prices_df[(wi_mask&(prices_df[\"month\"]==3))].index,'snow_m']=5\n",
    "prices_df.loc[prices_df[(wi_mask&(prices_df[\"month\"]==4))].index,'snow_m']=2\n",
    "prices_df.loc[prices_df[(wi_mask&(prices_df[\"month\"]==5))].index,'snow_m']=0\n",
    "prices_df.loc[prices_df[(wi_mask&(prices_df[\"month\"]==6))].index,'snow_m']=0\n",
    "prices_df.loc[prices_df[(wi_mask&(prices_df[\"month\"]==7))].index,'snow_m']=0\n",
    "prices_df.loc[prices_df[(wi_mask&(prices_df[\"month\"]==8))].index,'snow_m']=0\n",
    "prices_df.loc[prices_df[(wi_mask&(prices_df[\"month\"]==9))].index,'snow_m']=0\n",
    "prices_df.loc[prices_df[(wi_mask&(prices_df[\"month\"]==10))].index,'snow_m']=0\n",
    "prices_df.loc[prices_df[(wi_mask&(prices_df[\"month\"]==11))].index,'snow_m']=3\n",
    "prices_df.loc[prices_df[(wi_mask&(prices_df[\"month\"]==12))].index,'snow_m']=9\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ca_mask \n",
    "# temperature_con\n",
    "\n",
    "prices_df.loc[prices_df[(ca_mask&(prices_df[\"month\"]==1))].index,'temperature_con']=5\n",
    "prices_df.loc[prices_df[(ca_mask&(prices_df[\"month\"]==2))].index,'temperature_con']=5\n",
    "prices_df.loc[prices_df[(ca_mask&(prices_df[\"month\"]==3))].index,'temperature_con']=5\n",
    "prices_df.loc[prices_df[(ca_mask&(prices_df[\"month\"]==4))].index,'temperature_con']=5\n",
    "prices_df.loc[prices_df[(ca_mask&(prices_df[\"month\"]==5))].index,'temperature_con']=5\n",
    "prices_df.loc[prices_df[(ca_mask&(prices_df[\"month\"]==6))].index,'temperature_con']=6\n",
    "prices_df.loc[prices_df[(ca_mask&(prices_df[\"month\"]==7))].index,'temperature_con']=6\n",
    "prices_df.loc[prices_df[(ca_mask&(prices_df[\"month\"]==8))].index,'temperature_con']=6\n",
    "prices_df.loc[prices_df[(ca_mask&(prices_df[\"month\"]==9))].index,'temperature_con']=6\n",
    "prices_df.loc[prices_df[(ca_mask&(prices_df[\"month\"]==10))].index,'temperature_con']=6\n",
    "prices_df.loc[prices_df[(ca_mask&(prices_df[\"month\"]==11))].index,'temperature_con']=5\n",
    "prices_df.loc[prices_df[(ca_mask&(prices_df[\"month\"]==12))].index,'temperature_con']=5\n",
    "\n",
    "# temperature_high\n",
    "\n",
    "prices_df.loc[prices_df[(ca_mask&(prices_df[\"month\"]==1))].index,'temperature_high']=20\n",
    "prices_df.loc[prices_df[(ca_mask&(prices_df[\"month\"]==2))].index,'temperature_high']=20\n",
    "prices_df.loc[prices_df[(ca_mask&(prices_df[\"month\"]==3))].index,'temperature_high']=21\n",
    "prices_df.loc[prices_df[(ca_mask&(prices_df[\"month\"]==4))].index,'temperature_high']=22.5\n",
    "prices_df.loc[prices_df[(ca_mask&(prices_df[\"month\"]==5))].index,'temperature_high']=23.5\n",
    "prices_df.loc[prices_df[(ca_mask&(prices_df[\"month\"]==6))].index,'temperature_high']=25.5\n",
    "prices_df.loc[prices_df[(ca_mask&(prices_df[\"month\"]==7))].index,'temperature_high']=28\n",
    "prices_df.loc[prices_df[(ca_mask&(prices_df[\"month\"]==8))].index,'temperature_high']=29.5\n",
    "prices_df.loc[prices_df[(ca_mask&(prices_df[\"month\"]==9))].index,'temperature_high']=28.5\n",
    "prices_df.loc[prices_df[(ca_mask&(prices_df[\"month\"]==10))].index,'temperature_high']=25.5\n",
    "prices_df.loc[prices_df[(ca_mask&(prices_df[\"month\"]==11))].index,'temperature_high']=22\n",
    "prices_df.loc[prices_df[(ca_mask&(prices_df[\"month\"]==12))].index,'temperature_high']=20\n",
    "\n",
    "# rainfall_m\n",
    "\n",
    "prices_df.loc[prices_df[(ca_mask&(prices_df[\"month\"]==1))].index,'rainfall_m']=69\n",
    "prices_df.loc[prices_df[(ca_mask&(prices_df[\"month\"]==2))].index,'rainfall_m']=79\n",
    "prices_df.loc[prices_df[(ca_mask&(prices_df[\"month\"]==3))].index,'rainfall_m']=50\n",
    "prices_df.loc[prices_df[(ca_mask&(prices_df[\"month\"]==4))].index,'rainfall_m']=20\n",
    "prices_df.loc[prices_df[(ca_mask&(prices_df[\"month\"]==5))].index,'rainfall_m']=5\n",
    "prices_df.loc[prices_df[(ca_mask&(prices_df[\"month\"]==6))].index,'rainfall_m']=1.5\n",
    "prices_df.loc[prices_df[(ca_mask&(prices_df[\"month\"]==7))].index,'rainfall_m']=0\n",
    "prices_df.loc[prices_df[(ca_mask&(prices_df[\"month\"]==8))].index,'rainfall_m']=1\n",
    "prices_df.loc[prices_df[(ca_mask&(prices_df[\"month\"]==9))].index,'rainfall_m']=4\n",
    "prices_df.loc[prices_df[(ca_mask&(prices_df[\"month\"]==10))].index,'rainfall_m']=12\n",
    "prices_df.loc[prices_df[(ca_mask&(prices_df[\"month\"]==11))].index,'rainfall_m']=29\n",
    "prices_df.loc[prices_df[(ca_mask&(prices_df[\"month\"]==12))].index,'rainfall_m']=52\n",
    "\n",
    "# snow_m\n",
    "\n",
    "prices_df.loc[prices_df[(ca_mask&(prices_df[\"month\"]==1))].index,'snow_m']=0\n",
    "prices_df.loc[prices_df[(ca_mask&(prices_df[\"month\"]==2))].index,'snow_m']=0\n",
    "prices_df.loc[prices_df[(ca_mask&(prices_df[\"month\"]==3))].index,'snow_m']=0\n",
    "prices_df.loc[prices_df[(ca_mask&(prices_df[\"month\"]==4))].index,'snow_m']=0\n",
    "prices_df.loc[prices_df[(ca_mask&(prices_df[\"month\"]==5))].index,'snow_m']=0\n",
    "prices_df.loc[prices_df[(ca_mask&(prices_df[\"month\"]==6))].index,'snow_m']=0\n",
    "prices_df.loc[prices_df[(ca_mask&(prices_df[\"month\"]==7))].index,'snow_m']=0\n",
    "prices_df.loc[prices_df[(ca_mask&(prices_df[\"month\"]==8))].index,'snow_m']=0\n",
    "prices_df.loc[prices_df[(ca_mask&(prices_df[\"month\"]==9))].index,'snow_m']=0\n",
    "prices_df.loc[prices_df[(ca_mask&(prices_df[\"month\"]==10))].index,'snow_m']=0\n",
    "prices_df.loc[prices_df[(ca_mask&(prices_df[\"month\"]==11))].index,'snow_m']=0\n",
    "prices_df.loc[prices_df[(ca_mask&(prices_df[\"month\"]==12))].index,'snow_m']=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## tx_mask \n",
    "# temperature_con(气候概要)\n",
    "\n",
    "prices_df.loc[prices_df[(tx_mask&(prices_df[\"month\"]==1))].index,'temperature_con']=4\n",
    "prices_df.loc[prices_df[(tx_mask&(prices_df[\"month\"]==2))].index,'temperature_con']=4\n",
    "prices_df.loc[prices_df[(tx_mask&(prices_df[\"month\"]==3))].index,'temperature_con']=5\n",
    "prices_df.loc[prices_df[(tx_mask&(prices_df[\"month\"]==4))].index,'temperature_con']=6\n",
    "prices_df.loc[prices_df[(tx_mask&(prices_df[\"month\"]==5))].index,'temperature_con']=6\n",
    "prices_df.loc[prices_df[(tx_mask&(prices_df[\"month\"]==6))].index,'temperature_con']=7\n",
    "prices_df.loc[prices_df[(tx_mask&(prices_df[\"month\"]==7))].index,'temperature_con']=8\n",
    "prices_df.loc[prices_df[(tx_mask&(prices_df[\"month\"]==8))].index,'temperature_con']=8\n",
    "prices_df.loc[prices_df[(tx_mask&(prices_df[\"month\"]==9))].index,'temperature_con']=7\n",
    "prices_df.loc[prices_df[(tx_mask&(prices_df[\"month\"]==10))].index,'temperature_con']=6\n",
    "prices_df.loc[prices_df[(tx_mask&(prices_df[\"month\"]==11))].index,'temperature_con']=5\n",
    "prices_df.loc[prices_df[(tx_mask&(prices_df[\"month\"]==12))].index,'temperature_con']=4\n",
    "\n",
    "# temperature_high\n",
    "\n",
    "prices_df.loc[prices_df[(tx_mask&(prices_df[\"month\"]==1))].index,'temperature_high']=14\n",
    "prices_df.loc[prices_df[(tx_mask&(prices_df[\"month\"]==2))].index,'temperature_high']=17\n",
    "prices_df.loc[prices_df[(tx_mask&(prices_df[\"month\"]==3))].index,'temperature_high']=21\n",
    "prices_df.loc[prices_df[(tx_mask&(prices_df[\"month\"]==4))].index,'temperature_high']=24.5\n",
    "prices_df.loc[prices_df[(tx_mask&(prices_df[\"month\"]==5))].index,'temperature_high']=28.5\n",
    "prices_df.loc[prices_df[(tx_mask&(prices_df[\"month\"]==6))].index,'temperature_high']=32.5\n",
    "prices_df.loc[prices_df[(tx_mask&(prices_df[\"month\"]==7))].index,'temperature_high']=35\n",
    "prices_df.loc[prices_df[(tx_mask&(prices_df[\"month\"]==8))].index,'temperature_high']=35.5\n",
    "prices_df.loc[prices_df[(tx_mask&(prices_df[\"month\"]==9))].index,'temperature_high']=31\n",
    "prices_df.loc[prices_df[(tx_mask&(prices_df[\"month\"]==10))].index,'temperature_high']=26\n",
    "prices_df.loc[prices_df[(tx_mask&(prices_df[\"month\"]==11))].index,'temperature_high']=19\n",
    "prices_df.loc[prices_df[(tx_mask&(prices_df[\"month\"]==12))].index,'temperature_high']=15\n",
    "\n",
    "# rainfall_m\n",
    "\n",
    "prices_df.loc[prices_df[(tx_mask&(prices_df[\"month\"]==1))].index,'rainfall_m']=50\n",
    "prices_df.loc[prices_df[(tx_mask&(prices_df[\"month\"]==2))].index,'rainfall_m']=59\n",
    "prices_df.loc[prices_df[(tx_mask&(prices_df[\"month\"]==3))].index,'rainfall_m']=70\n",
    "prices_df.loc[prices_df[(tx_mask&(prices_df[\"month\"]==4))].index,'rainfall_m']=85\n",
    "prices_df.loc[prices_df[(tx_mask&(prices_df[\"month\"]==5))].index,'rainfall_m']=102\n",
    "prices_df.loc[prices_df[(tx_mask&(prices_df[\"month\"]==6))].index,'rainfall_m']=80\n",
    "prices_df.loc[prices_df[(tx_mask&(prices_df[\"month\"]==7))].index,'rainfall_m']=42\n",
    "prices_df.loc[prices_df[(tx_mask&(prices_df[\"month\"]==8))].index,'rainfall_m']=47\n",
    "prices_df.loc[prices_df[(tx_mask&(prices_df[\"month\"]==9))].index,'rainfall_m']=64\n",
    "prices_df.loc[prices_df[(tx_mask&(prices_df[\"month\"]==10))].index,'rainfall_m']=89\n",
    "prices_df.loc[prices_df[(tx_mask&(prices_df[\"month\"]==11))].index,'rainfall_m']=70\n",
    "prices_df.loc[prices_df[(tx_mask&(prices_df[\"month\"]==12))].index,'rainfall_m']=62\n",
    "\n",
    "# snow_m\n",
    "\n",
    "prices_df.loc[prices_df[(tx_mask&(prices_df[\"month\"]==1))].index,'snow_m']=0\n",
    "prices_df.loc[prices_df[(tx_mask&(prices_df[\"month\"]==2))].index,'snow_m']=0\n",
    "prices_df.loc[prices_df[(tx_mask&(prices_df[\"month\"]==3))].index,'snow_m']=0\n",
    "prices_df.loc[prices_df[(tx_mask&(prices_df[\"month\"]==4))].index,'snow_m']=0\n",
    "prices_df.loc[prices_df[(tx_mask&(prices_df[\"month\"]==5))].index,'snow_m']=0\n",
    "prices_df.loc[prices_df[(tx_mask&(prices_df[\"month\"]==6))].index,'snow_m']=0\n",
    "prices_df.loc[prices_df[(tx_mask&(prices_df[\"month\"]==7))].index,'snow_m']=0\n",
    "prices_df.loc[prices_df[(tx_mask&(prices_df[\"month\"]==8))].index,'snow_m']=0\n",
    "prices_df.loc[prices_df[(tx_mask&(prices_df[\"month\"]==9))].index,'snow_m']=0\n",
    "prices_df.loc[prices_df[(tx_mask&(prices_df[\"month\"]==10))].index,'snow_m']=0\n",
    "prices_df.loc[prices_df[(tx_mask&(prices_df[\"month\"]==11))].index,'snow_m']=0\n",
    "prices_df.loc[prices_df[(tx_mask&(prices_df[\"month\"]==12))].index,'snow_m']=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prices_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del prices_df['month'], prices_df['year']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prices_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Merge prices and save part 2')\n",
    "\n",
    "# Merge Prices\n",
    "original_columns = list(grid_df)\n",
    "grid_df = grid_df.merge(prices_df, on=['store_id','item_id','wm_yr_wk'], how='left')\n",
    "keep_columns = [col for col in list(grid_df) if col not in original_columns]\n",
    "grid_df = grid_df[MAIN_INDEX+keep_columns]\n",
    "grid_df = reduce_mem_usage(grid_df)\n",
    "\n",
    "# Safe part 2\n",
    "grid_df.to_pickle('grid_part_2.pkl')\n",
    "print('Size:', grid_df.shape)\n",
    "\n",
    "# We don't need prices_df anymore\n",
    "del prices_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# calender feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add calender feature\n",
    "calendar_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calendar_df[\"event_name_1\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calendar_df[\"event_type_1\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 上下半月\n",
    "calendar_df['date']=pd.to_datetime(calendar_df['date'],format=\"%Y-%m-%d\")\n",
    "# calendar_df['is_first_half_month']=\n",
    "calendar_df['is_first_half_month']=[ 1 if x.day>=15 else 0  for x in calendar_df['date']]\n",
    "calendar_df['is_first_half_month']=calendar_df['is_first_half_month'].astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calendar_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "周级别处理(wday ==1,2 是weekedn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# calendar_df=calendar_df.reset_index()\n",
    "calendar_df[\"weekend\"]=[ 1 if x<=2 else 0  for x in calendar_df['wday']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calendar_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 节日前后处理 event_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calendar_df[\"event_attention_after\"]=np.nan\n",
    "event_index=calendar_df[~calendar_df[\"event_name_1\"].isnull()].index\n",
    "for i in range(0,8):\n",
    "    if event_index[-1]+i>=calendar_df.shape[0]:\n",
    "        event_index=event_index[event_index<(event_index[-1]-i)]\n",
    "    calendar_df.loc[event_index+i,\"event_attention_after\"]=7-i\n",
    "calendar_df.loc[event_index,\"event_attention_after\"]=7\n",
    "event_index=calendar_df[~calendar_df[\"event_name_1\"].isnull()].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calendar_df.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calendar_df[\"event_attention_bef\"]=np.nan\n",
    "for i in range(0,8):\n",
    "    calendar_df.loc[calendar_df[~calendar_df[\"event_name_1\"].isnull()].index-i,\"event_attention_bef\"]=i*-1\n",
    "calendar_df.loc[calendar_df[~calendar_df[\"event_name_1\"].isnull()].index,\"event_attention_bef\"]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calendar_df[\"event_attention_sum\"]=calendar_df[\"event_attention_bef\"]+calendar_df[\"event_attention_after\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calendar_df[\"event_bef_weekend\"]=np.nan\n",
    "event_round_index=calendar_df[~calendar_df[\"event_attention_bef\"].isnull()]\n",
    "calendar_df.loc[event_round_index[event_round_index.wday<=2].index,\"event_bef_weekend\"]=1\n",
    "calendar_df.loc[event_index,\"event_bef_weekend\"]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calendar_df[\"event_after_weekend\"]=np.nan\n",
    "event_round_index=calendar_df[~calendar_df[\"event_attention_after\"].isnull()]\n",
    "calendar_df.loc[event_round_index[event_round_index.wday<=2].index,\"event_after_weekend\"]=1\n",
    "calendar_df.loc[event_index,\"event_after_weekend\"]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calendar_df.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calendar_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  NBA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 赛事数据\n",
    "calendar_df[\"NBA\"]=np.nan\n",
    "calendar_df[\"date\"]=pd.to_datetime(calendar_df[\"date\"])\n",
    "#http://www.uhchina.com/lanqiu/2013-2014nba/\n",
    "# http://sports.sina.com.cn/nba/playoff1213.html\n",
    "a=[[2011,5,31],[2011,6,2],[2011,6,5],[2011,6,7],[2011,6,9],[2011,6,12],\n",
    "   [2012,6,12],[2012,6,14],[2012,6,17],[2012,6,19],[2012,6,21],\n",
    "   [2013,6,6],[2013,6,9],[2013,6,11],[2013,6,13],[2013,6,16],[2013,6,18],[2013,6,20],\n",
    "   [2014,6,5],[2014,6,8],[2014,6,10],[2014,6,12],[2014,6,15], \n",
    "   [2015,6,4],[2015,6,7],[2015,6,9],[2015,6,11],[2015,6,14], [2015,6,16],\n",
    "   [2016,6,2],[2016,6,5],[2016,6,8],[2016,6,10],[2016,6,13],[2016,6,16] ,[2016,6,19]\n",
    "  ]\n",
    "for i in range(0,len(a)):\n",
    "    calendar_df.loc[calendar_df[calendar_df[\"date\"]==datetime.datetime(*a[i])].index,\"NBA\"]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "calendar_df[calendar_df[\"NBA\"]==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_df = grid_df[MAIN_INDEX]\n",
    "\n",
    "# Merge calendar partly\n",
    "icols = ['date',\n",
    "         'd',\n",
    "         #category\n",
    "         'event_name_1',\n",
    "         'event_type_1',\n",
    "         'event_name_2',\n",
    "         'event_type_2',\n",
    "         'snap_CA',\n",
    "         'snap_TX',\n",
    "         'snap_WI', \n",
    "         'is_first_half_month', \n",
    "         'event_bef_weekend', \n",
    "         'event_after_weekend',\n",
    "         'NBA',\n",
    "         \n",
    "         \n",
    "         # numerical\n",
    "        'event_attention_after',\n",
    "         'event_attention_bef',\n",
    "         'event_attention_sum',\n",
    "         \n",
    "\n",
    "\n",
    "        ]\n",
    "grid_df = grid_df.merge(calendar_df[icols], on=['d'], how='left')\n",
    "\n",
    "# Minify data\n",
    "# 'snap_' columns we can convert to bool or int8\n",
    "icols = ['event_name_1',\n",
    "         'event_type_1',\n",
    "         'event_name_2',\n",
    "         'event_type_2',\n",
    "         'snap_CA',\n",
    "         'snap_TX',\n",
    "         'snap_WI', \n",
    "         'is_first_half_month', \n",
    "         'event_bef_weekend', \n",
    "         'event_after_weekend',\n",
    "         'NBA']\n",
    "for col in icols:\n",
    "    grid_df[col] = grid_df[col].astype('category')\n",
    "\n",
    "grid_df['event_attention_after'] = grid_df['event_attention_after'].astype(np.float16) #v2\n",
    "grid_df['event_attention_bef'] = grid_df['event_attention_bef'].astype(np.float16) #v2 \n",
    "grid_df['event_attention_sum'] = grid_df['event_attention_sum'].astype(np.float16) #v2 \n",
    "    \n",
    "    \n",
    "    \n",
    "# Convert to DateTime\n",
    "grid_df['date'] = pd.to_datetime(grid_df['date'])\n",
    "\n",
    "# Make some features from date\n",
    "grid_df['tm_d'] = grid_df['date'].dt.day.astype(np.int8) #day of month\n",
    "grid_df['tm_w'] = grid_df['date'].dt.week.astype(np.int8) # week of year\n",
    "grid_df['tm_m'] = grid_df['date'].dt.month.astype(np.int8) # month of year\n",
    "grid_df['tm_q'] = grid_df['date'].dt.quarter.astype(np.int8) # quarter of year\n",
    "grid_df['tm_y'] = grid_df['date'].dt.year # year\n",
    "grid_df['tm_y'] = (grid_df['tm_y'] - grid_df['tm_y'].min()).astype(np.int8) # minus min(year)\n",
    "grid_df['tm_wm'] = grid_df['tm_d'].apply(lambda x: ceil(x/7)).astype(np.int8) # week of month\n",
    "\n",
    "grid_df['tm_dw'] = grid_df['date'].dt.dayofweek.astype(np.int8) # day of week: monday start from 0, so saturday. is 5\n",
    "grid_df['tm_w_end'] = (grid_df['tm_dw']>=5).astype(np.int8) # weekend and weekday\n",
    "\n",
    "# Remove date\n",
    "del grid_df['date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Save part 3')\n",
    "\n",
    "# Safe part 3\n",
    "grid_df.to_pickle('grid_part_3.pkl')\n",
    "print('Size:', grid_df.shape)\n",
    "\n",
    "# We don't need calendar_df anymore\n",
    "del calendar_df\n",
    "del grid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Part 1\n",
    "# Convert 'd' to int\n",
    "grid_df = pd.read_pickle('grid_part_1.pkl')\n",
    "grid_df['d'] = grid_df['d'].apply(lambda x: x[2:]).astype(np.int16)\n",
    "\n",
    "# Remove 'wm_yr_wk'\n",
    "del grid_df['wm_yr_wk']\n",
    "print(grid_df.info())\n",
    "grid_df.to_pickle('grid_part_1.pkl')\n",
    "\n",
    "del grid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group aggregation\n",
    "grid_df = pd.read_pickle('grid_part_1.pkl')\n",
    "grid_df[TARGET][grid_df['d']>1913] = np.nan # change this part for final\n",
    "\n",
    "base_cols = list(grid_df)\n",
    "\n",
    "print(base_cols)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "icols =  [\n",
    "            ['state_id'],\n",
    "            ['store_id'],\n",
    "            ['cat_id'],\n",
    "            ['dept_id'],\n",
    "            ['state_id', 'cat_id'],\n",
    "            ['state_id', 'dept_id'],\n",
    "            ['store_id', 'cat_id'],\n",
    "            ['store_id', 'dept_id'],\n",
    "            ['item_id'],\n",
    "            ['item_id', 'state_id'],\n",
    "            ['item_id', 'store_id'],\n",
    "    \n",
    "            ]\n",
    "\n",
    "for col in icols:\n",
    "    print('Encoding', col)\n",
    "    col_name = '_'+'_'.join(col)+'_'\n",
    "    grid_df['enc'+col_name+'mean'] = grid_df.groupby(col)[TARGET].transform('mean').astype(np.float16)\n",
    "    grid_df['enc'+col_name+'std'] = grid_df.groupby(col)[TARGET].transform('std').astype(np.float16)\n",
    "\n",
    "    \n",
    "keep_cols = [col for col in list(grid_df) if col not in base_cols]\n",
    "grid_df = grid_df[['id','d']+keep_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Save Mean/Std encoding')\n",
    "grid_df.to_pickle('mean_encoding_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new lag data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('sales_train_evaluation.csv') # change this part for final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Create Grid')\n",
    "\n",
    "# We can tranform horizontal representation \n",
    "# to vertical \"view\"\n",
    "# Our \"index\" will be 'id','item_id','dept_id','cat_id','store_id','state_id'\n",
    "# and labels are 'd_' coulmns\n",
    "\n",
    "index_columns = ['id','item_id','dept_id','cat_id','store_id','state_id']\n",
    "grid_df = pd.melt(train_df, \n",
    "                  id_vars = index_columns, \n",
    "                  var_name = 'd', \n",
    "                  value_name = TARGET)\n",
    "\n",
    "# If we look on train_df we see that \n",
    "# we don't have a lot of traning rows\n",
    "# but each day can provide more train data\n",
    "print('Train rows:', len(train_df), len(grid_df))\n",
    "\n",
    "# To be able to make predictions\n",
    "# we need to add \"test set\" to our grid\n",
    "add_grid = pd.DataFrame()\n",
    "for i in range(1,29):\n",
    "    temp_df = train_df[index_columns]\n",
    "    temp_df = temp_df.drop_duplicates()\n",
    "    temp_df['d'] = 'd_'+ str(END_TRAIN+i)\n",
    "    temp_df[TARGET] = np.nan\n",
    "    add_grid = pd.concat([add_grid,temp_df])\n",
    "\n",
    "grid_df = pd.concat([grid_df,add_grid])\n",
    "grid_df = grid_df.reset_index(drop=True)\n",
    "\n",
    "# Remove some temoprary DFs\n",
    "del temp_df, add_grid\n",
    "\n",
    "# We will not need original train_df\n",
    "# anymore and can remove it\n",
    "del train_df\n",
    "\n",
    "# You don't have to use df = df construction\n",
    "# you can use inplace=True instead.\n",
    "# like this\n",
    "# grid_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Let's check our memory usage\n",
    "print(\"{:>20}: {:>8}\".format('Original grid_df',sizeof_fmt(grid_df.memory_usage(index=True).sum())))\n",
    "\n",
    "# We can free some memory \n",
    "# by converting \"strings\" to categorical\n",
    "# it will not affect merging and \n",
    "# we will not lose any valuable data\n",
    "for col in ['id','item_id','dept_id','cat_id','store_id','state_id']:\n",
    "    grid_df[col] = grid_df[col].astype('category')\n",
    "\n",
    "# Let's check again memory usage\n",
    "print(\"{:>20}: {:>8}\".format('Reduced grid_df',sizeof_fmt(grid_df.memory_usage(index=True).sum())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need only 'id','d','sales'\n",
    "# to make lags and rollings\n",
    "grid_df = grid_df[['id','d','sales']]\n",
    "SHIFT_DAY = 28\n",
    "# Lags\n",
    "# with 28 day shift\n",
    "start_time = time.time()\n",
    "print('Create lags')\n",
    "\n",
    "LAG_DAYS = [col for col in range(SHIFT_DAY,SHIFT_DAY+15)]\n",
    "grid_df = grid_df.assign(**{\n",
    "        '{}_lag_{}'.format(col, l): grid_df.groupby(['id'])[col].transform(lambda x: x.shift(l))\n",
    "        for l in LAG_DAYS\n",
    "        for col in [TARGET]\n",
    "    })\n",
    "\n",
    "# Minify lag columns\n",
    "for col in list(grid_df):\n",
    "    if 'lag' in col:\n",
    "        grid_df[col] = grid_df[col].astype(np.float16)\n",
    "\n",
    "print('%0.2f min: Lags' % ((time.time() - start_time) / 60))\n",
    "\n",
    "# Rollings\n",
    "# with 28 day shift\n",
    "start_time = time.time()\n",
    "print('Create rolling aggs')\n",
    "\n",
    "for i in [7,14,28,56,168]:\n",
    "    print('Rolling period:', i)\n",
    "    grid_df['rolling_mean_'+str(i)] = grid_df.groupby(['id'])[TARGET].transform(lambda x: x.shift(SHIFT_DAY).rolling(i).mean()).astype(np.float16)\n",
    "    grid_df['rolling_std_'+str(i)]  = grid_df.groupby(['id'])[TARGET].transform(lambda x: x.shift(SHIFT_DAY).rolling(i).std()).astype(np.float16)\n",
    "\n",
    "# Rollings\n",
    "# with sliding shift\n",
    "for d_shift in [1,7,14]: \n",
    "    print('Shifting period:', d_shift)\n",
    "    for d_window in [7,14,28,56]:\n",
    "        col_name = 'rolling_mean_tmp_'+str(d_shift)+'_'+str(d_window)\n",
    "        grid_df[col_name] = grid_df.groupby(['id'])[TARGET].transform(lambda x: x.shift(d_shift).rolling(d_window).mean()).astype(np.float16)\n",
    "    \n",
    "    \n",
    "print('%0.2f min: Lags' % ((time.time() - start_time) / 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add quantile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [28,56,168]:\n",
    "    print('Rolling quantile period:', i)\n",
    "    grid_df['rolling_quantile_97_'+str(i)] = grid_df.groupby(['id'])[TARGET].transform(lambda x: x.shift(SHIFT_DAY).rolling(i).quantile(0.97, interpolation='midpoint')).astype(np.float16)\n",
    "    grid_df['rolling_quantile_87.5_'+str(i)]  = grid_df.groupby(['id'])[TARGET].transform(lambda x: x.shift(SHIFT_DAY).rolling(i).quantile(0.875, interpolation='midpoint')).astype(np.float16)\n",
    "    grid_df['rolling_quantile_50_'+str(i)] = grid_df.groupby(['id'])[TARGET].transform(lambda x: x.shift(SHIFT_DAY).rolling(i).quantile(0.5, interpolation='midpoint')).astype(np.float16)\n",
    "    grid_df['rolling_quantile_22.5_'+str(i)]  = grid_df.groupby(['id'])[TARGET].transform(lambda x: x.shift(SHIFT_DAY).rolling(i).quantile(0.225, interpolation='midpoint')).astype(np.float16)\n",
    "    grid_df['rolling_quantile_3_'+str(i)]  = grid_df.groupby(['id'])[TARGET].transform(lambda x: x.shift(SHIFT_DAY).rolling(i).quantile(0.03, interpolation='midpoint')).astype(np.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_df['d']=grid_df['d'].apply(lambda x:int(x.split('_')[1]))\n",
    "grid_df_1 = pd.read_pickle('grid_part_1.pkl')\n",
    "grid_df_1=grid_df_1[['id','d']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_df_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_df=grid_df_1.merge(grid_df, on=['id','d'], how='left' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Save lags and rollings')\n",
    "grid_df.to_pickle('lags_df_'+str(SHIFT_DAY)+'_v3.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
